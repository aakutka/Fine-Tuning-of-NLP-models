{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embeddings\n",
      "embeddings.word_embeddings\n",
      "embeddings.position_embeddings\n",
      "embeddings.token_type_embeddings\n",
      "embeddings.LayerNorm\n",
      "embeddings.dropout\n",
      "encoder\n",
      "encoder.layer\n",
      "encoder.layer.0\n",
      "encoder.layer.0.attention\n",
      "encoder.layer.0.attention.self\n",
      "encoder.layer.0.attention.self.query\n",
      "encoder.layer.0.attention.self.key\n",
      "encoder.layer.0.attention.self.value\n",
      "encoder.layer.0.attention.self.dropout\n",
      "encoder.layer.0.attention.output\n",
      "encoder.layer.0.attention.output.dense\n",
      "encoder.layer.0.attention.output.LayerNorm\n",
      "encoder.layer.0.attention.output.dropout\n",
      "encoder.layer.0.intermediate\n",
      "encoder.layer.0.intermediate.dense\n",
      "encoder.layer.0.intermediate.intermediate_act_fn\n",
      "encoder.layer.0.output\n",
      "encoder.layer.0.output.dense\n",
      "encoder.layer.0.output.LayerNorm\n",
      "encoder.layer.0.output.dropout\n",
      "encoder.layer.1\n",
      "encoder.layer.1.attention\n",
      "encoder.layer.1.attention.self\n",
      "encoder.layer.1.attention.self.query\n",
      "encoder.layer.1.attention.self.key\n",
      "encoder.layer.1.attention.self.value\n",
      "encoder.layer.1.attention.self.dropout\n",
      "encoder.layer.1.attention.output\n",
      "encoder.layer.1.attention.output.dense\n",
      "encoder.layer.1.attention.output.LayerNorm\n",
      "encoder.layer.1.attention.output.dropout\n",
      "encoder.layer.1.intermediate\n",
      "encoder.layer.1.intermediate.dense\n",
      "encoder.layer.1.intermediate.intermediate_act_fn\n",
      "encoder.layer.1.output\n",
      "encoder.layer.1.output.dense\n",
      "encoder.layer.1.output.LayerNorm\n",
      "encoder.layer.1.output.dropout\n",
      "encoder.layer.2\n",
      "encoder.layer.2.attention\n",
      "encoder.layer.2.attention.self\n",
      "encoder.layer.2.attention.self.query\n",
      "encoder.layer.2.attention.self.key\n",
      "encoder.layer.2.attention.self.value\n",
      "encoder.layer.2.attention.self.dropout\n",
      "encoder.layer.2.attention.output\n",
      "encoder.layer.2.attention.output.dense\n",
      "encoder.layer.2.attention.output.LayerNorm\n",
      "encoder.layer.2.attention.output.dropout\n",
      "encoder.layer.2.intermediate\n",
      "encoder.layer.2.intermediate.dense\n",
      "encoder.layer.2.intermediate.intermediate_act_fn\n",
      "encoder.layer.2.output\n",
      "encoder.layer.2.output.dense\n",
      "encoder.layer.2.output.LayerNorm\n",
      "encoder.layer.2.output.dropout\n",
      "encoder.layer.3\n",
      "encoder.layer.3.attention\n",
      "encoder.layer.3.attention.self\n",
      "encoder.layer.3.attention.self.query\n",
      "encoder.layer.3.attention.self.key\n",
      "encoder.layer.3.attention.self.value\n",
      "encoder.layer.3.attention.self.dropout\n",
      "encoder.layer.3.attention.output\n",
      "encoder.layer.3.attention.output.dense\n",
      "encoder.layer.3.attention.output.LayerNorm\n",
      "encoder.layer.3.attention.output.dropout\n",
      "encoder.layer.3.intermediate\n",
      "encoder.layer.3.intermediate.dense\n",
      "encoder.layer.3.intermediate.intermediate_act_fn\n",
      "encoder.layer.3.output\n",
      "encoder.layer.3.output.dense\n",
      "encoder.layer.3.output.LayerNorm\n",
      "encoder.layer.3.output.dropout\n",
      "encoder.layer.4\n",
      "encoder.layer.4.attention\n",
      "encoder.layer.4.attention.self\n",
      "encoder.layer.4.attention.self.query\n",
      "encoder.layer.4.attention.self.key\n",
      "encoder.layer.4.attention.self.value\n",
      "encoder.layer.4.attention.self.dropout\n",
      "encoder.layer.4.attention.output\n",
      "encoder.layer.4.attention.output.dense\n",
      "encoder.layer.4.attention.output.LayerNorm\n",
      "encoder.layer.4.attention.output.dropout\n",
      "encoder.layer.4.intermediate\n",
      "encoder.layer.4.intermediate.dense\n",
      "encoder.layer.4.intermediate.intermediate_act_fn\n",
      "encoder.layer.4.output\n",
      "encoder.layer.4.output.dense\n",
      "encoder.layer.4.output.LayerNorm\n",
      "encoder.layer.4.output.dropout\n",
      "encoder.layer.5\n",
      "encoder.layer.5.attention\n",
      "encoder.layer.5.attention.self\n",
      "encoder.layer.5.attention.self.query\n",
      "encoder.layer.5.attention.self.key\n",
      "encoder.layer.5.attention.self.value\n",
      "encoder.layer.5.attention.self.dropout\n",
      "encoder.layer.5.attention.output\n",
      "encoder.layer.5.attention.output.dense\n",
      "encoder.layer.5.attention.output.LayerNorm\n",
      "encoder.layer.5.attention.output.dropout\n",
      "encoder.layer.5.intermediate\n",
      "encoder.layer.5.intermediate.dense\n",
      "encoder.layer.5.intermediate.intermediate_act_fn\n",
      "encoder.layer.5.output\n",
      "encoder.layer.5.output.dense\n",
      "encoder.layer.5.output.LayerNorm\n",
      "encoder.layer.5.output.dropout\n",
      "encoder.layer.6\n",
      "encoder.layer.6.attention\n",
      "encoder.layer.6.attention.self\n",
      "encoder.layer.6.attention.self.query\n",
      "encoder.layer.6.attention.self.key\n",
      "encoder.layer.6.attention.self.value\n",
      "encoder.layer.6.attention.self.dropout\n",
      "encoder.layer.6.attention.output\n",
      "encoder.layer.6.attention.output.dense\n",
      "encoder.layer.6.attention.output.LayerNorm\n",
      "encoder.layer.6.attention.output.dropout\n",
      "encoder.layer.6.intermediate\n",
      "encoder.layer.6.intermediate.dense\n",
      "encoder.layer.6.intermediate.intermediate_act_fn\n",
      "encoder.layer.6.output\n",
      "encoder.layer.6.output.dense\n",
      "encoder.layer.6.output.LayerNorm\n",
      "encoder.layer.6.output.dropout\n",
      "encoder.layer.7\n",
      "encoder.layer.7.attention\n",
      "encoder.layer.7.attention.self\n",
      "encoder.layer.7.attention.self.query\n",
      "encoder.layer.7.attention.self.key\n",
      "encoder.layer.7.attention.self.value\n",
      "encoder.layer.7.attention.self.dropout\n",
      "encoder.layer.7.attention.output\n",
      "encoder.layer.7.attention.output.dense\n",
      "encoder.layer.7.attention.output.LayerNorm\n",
      "encoder.layer.7.attention.output.dropout\n",
      "encoder.layer.7.intermediate\n",
      "encoder.layer.7.intermediate.dense\n",
      "encoder.layer.7.intermediate.intermediate_act_fn\n",
      "encoder.layer.7.output\n",
      "encoder.layer.7.output.dense\n",
      "encoder.layer.7.output.LayerNorm\n",
      "encoder.layer.7.output.dropout\n",
      "encoder.layer.8\n",
      "encoder.layer.8.attention\n",
      "encoder.layer.8.attention.self\n",
      "encoder.layer.8.attention.self.query\n",
      "encoder.layer.8.attention.self.key\n",
      "encoder.layer.8.attention.self.value\n",
      "encoder.layer.8.attention.self.dropout\n",
      "encoder.layer.8.attention.output\n",
      "encoder.layer.8.attention.output.dense\n",
      "encoder.layer.8.attention.output.LayerNorm\n",
      "encoder.layer.8.attention.output.dropout\n",
      "encoder.layer.8.intermediate\n",
      "encoder.layer.8.intermediate.dense\n",
      "encoder.layer.8.intermediate.intermediate_act_fn\n",
      "encoder.layer.8.output\n",
      "encoder.layer.8.output.dense\n",
      "encoder.layer.8.output.LayerNorm\n",
      "encoder.layer.8.output.dropout\n",
      "encoder.layer.9\n",
      "encoder.layer.9.attention\n",
      "encoder.layer.9.attention.self\n",
      "encoder.layer.9.attention.self.query\n",
      "encoder.layer.9.attention.self.key\n",
      "encoder.layer.9.attention.self.value\n",
      "encoder.layer.9.attention.self.dropout\n",
      "encoder.layer.9.attention.output\n",
      "encoder.layer.9.attention.output.dense\n",
      "encoder.layer.9.attention.output.LayerNorm\n",
      "encoder.layer.9.attention.output.dropout\n",
      "encoder.layer.9.intermediate\n",
      "encoder.layer.9.intermediate.dense\n",
      "encoder.layer.9.intermediate.intermediate_act_fn\n",
      "encoder.layer.9.output\n",
      "encoder.layer.9.output.dense\n",
      "encoder.layer.9.output.LayerNorm\n",
      "encoder.layer.9.output.dropout\n",
      "encoder.layer.10\n",
      "encoder.layer.10.attention\n",
      "encoder.layer.10.attention.self\n",
      "encoder.layer.10.attention.self.query\n",
      "encoder.layer.10.attention.self.key\n",
      "encoder.layer.10.attention.self.value\n",
      "encoder.layer.10.attention.self.dropout\n",
      "encoder.layer.10.attention.output\n",
      "encoder.layer.10.attention.output.dense\n",
      "encoder.layer.10.attention.output.LayerNorm\n",
      "encoder.layer.10.attention.output.dropout\n",
      "encoder.layer.10.intermediate\n",
      "encoder.layer.10.intermediate.dense\n",
      "encoder.layer.10.intermediate.intermediate_act_fn\n",
      "encoder.layer.10.output\n",
      "encoder.layer.10.output.dense\n",
      "encoder.layer.10.output.LayerNorm\n",
      "encoder.layer.10.output.dropout\n",
      "encoder.layer.11\n",
      "encoder.layer.11.attention\n",
      "encoder.layer.11.attention.self\n",
      "encoder.layer.11.attention.self.query\n",
      "encoder.layer.11.attention.self.key\n",
      "encoder.layer.11.attention.self.value\n",
      "encoder.layer.11.attention.self.dropout\n",
      "encoder.layer.11.attention.output\n",
      "encoder.layer.11.attention.output.dense\n",
      "encoder.layer.11.attention.output.LayerNorm\n",
      "encoder.layer.11.attention.output.dropout\n",
      "encoder.layer.11.intermediate\n",
      "encoder.layer.11.intermediate.dense\n",
      "encoder.layer.11.intermediate.intermediate_act_fn\n",
      "encoder.layer.11.output\n",
      "encoder.layer.11.output.dense\n",
      "encoder.layer.11.output.LayerNorm\n",
      "encoder.layer.11.output.dropout\n",
      "pooler\n",
      "pooler.dense\n",
      "pooler.activation\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    print(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 109482240\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embeddings.word_embeddings.weight | Size: torch.Size([30522, 768]) | Requires Grad: True\n",
      "Layer: embeddings.position_embeddings.weight | Size: torch.Size([512, 768]) | Requires Grad: True\n",
      "Layer: embeddings.token_type_embeddings.weight | Size: torch.Size([2, 768]) | Requires Grad: True\n",
      "Layer: embeddings.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: embeddings.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: pooler.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: pooler.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Requires Grad: {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embeddings.word_embeddings.weight | Size: torch.Size([30522, 768]) | Requires Grad: True\n",
      "Layer: embeddings.position_embeddings.weight | Size: torch.Size([512, 768]) | Requires Grad: True\n",
      "Layer: embeddings.token_type_embeddings.weight | Size: torch.Size([2, 768]) | Requires Grad: True\n",
      "Layer: embeddings.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: embeddings.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.0.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.1.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.2.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.3.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.4.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.5.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.6.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.7.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.8.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.9.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.10.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.query.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.query.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.key.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.key.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.value.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.self.value.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.output.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.intermediate.dense.bias | Size: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.output.dense.weight | Size: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.output.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.output.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: encoder.layer.11.output.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
      "Layer: pooler.dense.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: pooler.dense.bias | Size: torch.Size([768]) | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Requires Grad: {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                               Text\n",
       "0   neutral  Technopolis plans to develop in stages an area...\n",
       "1  negative  The international electronic industry company ...\n",
       "2  positive  With the new production plant the company woul...\n",
       "3  positive  According to the company 's updated strategy f...\n",
       "4  positive  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"/home/aakash/Documents/COding/BerFIneTuning/archive/all-data.csv\",encoding='ISO-8859-1')\n",
    "df.columns=['Sentiment','Text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment    0\n",
       "Text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(x='Sentiment',data=df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 ... 0 0 0]\n",
      "['Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .'\n",
      " 'The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .'\n",
      " 'With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .'\n",
      " ...\n",
      " 'Operating profit fell to EUR 35.4 mn from EUR 68.8 mn in 2007 , including vessel sales gain of EUR 12.3 mn .'\n",
      " 'Net sales of the Paper segment decreased to EUR 221.6 mn in the second quarter of 2009 from EUR 241.1 mn in the second quarter of 2008 , while operating profit excluding non-recurring items rose to EUR 8.0 mn from EUR 7.6 mn .'\n",
      " 'Sales in Finland decreased by 10.5 % in January , while sales outside Finland dropped by 17 % .']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le=LabelEncoder()\n",
    "df['Sentiment']=le.fit_transform(df['Sentiment'])\n",
    "df.head()\n",
    "\n",
    "x=df['Sentiment'].values\n",
    "Y=df['Text'].values\n",
    "print(x)\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text,val_text,train_label,val_label=train_test_split(df['Text'],df['Sentiment'],test_size=0.2,random_state=42)\n",
    "\n",
    "tokeizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(train_text[0])\n",
    "print(type(train_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "maxi=128\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer(list(text),padding=True,truncation=True,max_length=maxi,return_tensors='pt')\n",
    "\n",
    "\n",
    "train_encoding=tokenize(train_text)\n",
    "val_encoding=tokenize(val_text)\n",
    "\n",
    "print(type(train_encoding))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.FinancialPhrase object at 0x78ae097d6ee0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====epoch1 / 2\n",
      "TRaning=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/243 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3765",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 3765",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m train_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     52\u001b[0m train_accuray\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train,desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     55\u001b[0m     batch\u001b[38;5;241m=\u001b[39m{k:v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     56\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m, in \u001b[0;36mFinancialPhrase.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,idx):\n\u001b[1;32m     18\u001b[0m     item\u001b[38;5;241m=\u001b[39m{key:tensor[idx] \u001b[38;5;28;01mfor\u001b[39;00m key,tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodings\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 19\u001b[0m     item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 3765"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizer,BertForSequenceClassification,AdamW,get_linear_schedule_with_warmup\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FinancialPhrase:\n",
    "    def __init__(self,encoding,labels):\n",
    "        self.encodings=encoding\n",
    "        self.labels=labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        item={key:tensor[idx] for key,tensor in self.encodings.items()}\n",
    "        item['labels']=torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "\n",
    "train_dataset=FinancialPhrase(train_encoding,train_label)\n",
    "val_dataset=FinancialPhrase(val_encoding,val_label)\n",
    "print(train_dataset)\n",
    "batch=16\n",
    "\n",
    "train=DataLoader(train_dataset,batch_size=batch,shuffle=True)\n",
    "val=DataLoader(val_dataset,batch_size=batch,shuffle=True)\n",
    "model=BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=3,output_attentions=False,output_hidden_states=False)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer=AdamW(model.parameters(),lr=2e-5,eps=1e-8)\n",
    "epoch=2\n",
    "total_steps=len(train)*epoch\n",
    "\n",
    "scheduler=get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "\n",
    "def flat_accuracy(preds,labels):\n",
    "    pred_flat=np.argmax(preds,axis=1).flatten()\n",
    "    labelsflat=np.argmax(preds,axis=1).flatten()\n",
    "    return np.sum(pred_flat == labelsflat)/len(labelsflat)\n",
    "\n",
    "for ep in range(epoch):\n",
    "\n",
    "    print(f'\\n=====epoch{ep+1} / {epoch}')\n",
    "    print('TRaning=====')\n",
    "\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    train_accuray=0\n",
    "\n",
    "    for batch in tqdm(train,desc='Training'):\n",
    "        batch={k:v.to(device) for k,v in batch.items()}\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs=model(**batch)\n",
    "        loss=outputs.loss\n",
    "        logits=outputs.logits\n",
    "        train_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=0.1)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        logits=logits.detach().cpu().numpy()\n",
    "        label_ids=batch['labels'].cpu().numpy()\n",
    "        train_accuray+=flat_accuracy(logits,label_ids)\n",
    "    avg_train_loss=train_loss/len(train)\n",
    "    avg_train_accuracy=train_accuray/len(train)\n",
    "\n",
    "    print(f\"TRaingloss: {avg_train_loss:.4f}\")\n",
    "    print(f\"TRainign accuracy: {avg_train_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Running Validation\")\n",
    "    model.eval()\n",
    "    total_val_loss=0\n",
    "    total_val_accuracy=0\n",
    "    all_preds=[]\n",
    "    all_labels=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val,des='Validation'):\n",
    "            batch={k:v.to(device) for k,v in batch.items()}\n",
    "            outputs=model(**batch)\n",
    "            loss=outputs.loss\n",
    "            logits=outputs.logits\n",
    "            total_val_loss+=loss.item()\n",
    "            logits=logits.detach().cpu().numpy()\n",
    "            label_ids=batch['labels'].cpu().numpy()\n",
    "            total_val_accuracy+=flat_accuracy(logits,label_ids)\n",
    "\n",
    "            all_preds.extend(np.argmax(logits,axis=1).flatten())\n",
    "            all_labels.extend(label_ids.flatten())\n",
    "    avg_val_loss=total_val_loss/len(val)\n",
    "    avg_val_accuracy=total_val_accuracy/len(val)\n",
    "    print(f\"Validation loss : {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation accuracy : {avg_train_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"classifcication report\")\n",
    "    print(classification_report(all_labels,all_preds))\n",
    "with torch.no_grad():\n",
    "    for batch in val:\n",
    "        batch={k:v.to(device) for k,v in batch.items()}\n",
    "        outputs=model(**batch)\n",
    "        logits=outputs.logits\n",
    "        preds=torch.argmax(logits,dim=1).cpu().numpy()\n",
    "        labels=batch['labels'].cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "accuracy=accuracy_score(all_labels,all_preds)\n",
    "print(f'\\nFinal Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "print(\"classification report\")\n",
    "print(classification_report(all_labels,all_preds))\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "  Sentiment                                           Sentence\n",
      "0   neutral  Technopolis plans to develop in stages an area...\n",
      "1  negative  The international electronic industry company ...\n",
      "2  positive  With the new production plant the company woul...\n",
      "3  positive  According to the company 's updated strategy f...\n",
      "4  positive  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 6 ========\n",
      "Training...\n",
      "\n",
      "Unfroze the top 2 layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 273/273 [01:51<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.8836\n",
      "Average training accuracy: 0.5694\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|| 31/31 [00:09<00:00,  3.33it/s]\n",
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5741\n",
      "Validation Accuracy: 0.7815\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.66      0.63      0.65       136\n",
      "    negative       0.80      0.72      0.76        61\n",
      "     neutral       0.84      0.87      0.85       288\n",
      "\n",
      "    accuracy                           0.79       485\n",
      "   macro avg       0.77      0.74      0.75       485\n",
      "weighted avg       0.78      0.79      0.78       485\n",
      "\n",
      "\n",
      "======== Epoch 2 / 6 ========\n",
      "Training...\n",
      "\n",
      "Unfroze the top 4 layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 273/273 [02:08<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.5068\n",
      "Average training accuracy: 0.7960\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|| 31/31 [00:08<00:00,  3.58it/s]\n",
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3887\n",
      "Validation Accuracy: 0.8484\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.78      0.80      0.79       136\n",
      "    negative       0.79      0.90      0.84        61\n",
      "     neutral       0.90      0.86      0.88       288\n",
      "\n",
      "    accuracy                           0.85       485\n",
      "   macro avg       0.82      0.85      0.84       485\n",
      "weighted avg       0.85      0.85      0.85       485\n",
      "\n",
      "\n",
      "======== Epoch 3 / 6 ========\n",
      "Training...\n",
      "\n",
      "Unfroze the top 6 layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 273/273 [02:44<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.3542\n",
      "Average training accuracy: 0.8624\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|| 31/31 [00:07<00:00,  4.17it/s]\n",
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3799\n",
      "Validation Accuracy: 0.8403\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.80      0.70      0.75       136\n",
      "    negative       0.75      0.90      0.82        61\n",
      "     neutral       0.88      0.90      0.89       288\n",
      "\n",
      "    accuracy                           0.84       485\n",
      "   macro avg       0.81      0.83      0.82       485\n",
      "weighted avg       0.84      0.84      0.84       485\n",
      "\n",
      "\n",
      "======== Epoch 4 / 6 ========\n",
      "Training...\n",
      "\n",
      "Unfroze the top 8 layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 273/273 [01:08<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.2639\n",
      "Average training accuracy: 0.9064\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|| 31/31 [00:03<00:00,  9.78it/s]\n",
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3787\n",
      "Validation Accuracy: 0.8665\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.85      0.76      0.80       136\n",
      "    negative       0.87      0.87      0.87        61\n",
      "     neutral       0.88      0.92      0.90       288\n",
      "\n",
      "    accuracy                           0.87       485\n",
      "   macro avg       0.86      0.85      0.86       485\n",
      "weighted avg       0.87      0.87      0.87       485\n",
      "\n",
      "\n",
      "======== Epoch 5 / 6 ========\n",
      "Training...\n",
      "\n",
      "Unfroze the top 10 layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 273/273 [01:18<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.1980\n",
      "Average training accuracy: 0.9281\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|| 31/31 [00:03<00:00,  9.77it/s]\n",
      "/home/aakash/miniconda3/envs/test/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4456\n",
      "Validation Accuracy: 0.8403\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.72      0.88      0.79       136\n",
      "    negative       0.83      0.87      0.85        61\n",
      "     neutral       0.92      0.82      0.87       288\n",
      "\n",
      "    accuracy                           0.84       485\n",
      "   macro avg       0.82      0.85      0.84       485\n",
      "weighted avg       0.85      0.84      0.84       485\n",
      "\n",
      "\n",
      "======== Epoch 6 / 6 ========\n",
      "Training...\n",
      "\n",
      "Unfroze the top 12 layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 273/273 [01:29<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.1417\n",
      "Average training accuracy: 0.9574\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|| 31/31 [00:03<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5400\n",
      "Validation Accuracy: 0.8625\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.86      0.81       136\n",
      "    negative       0.85      0.87      0.86        61\n",
      "     neutral       0.92      0.86      0.89       288\n",
      "\n",
      "    accuracy                           0.86       485\n",
      "   macro avg       0.85      0.86      0.85       485\n",
      "weighted avg       0.87      0.86      0.87       485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load the dataset\n",
    "#df = pd.read_csv('path_to_your_financial_phrase_bank.csv')\n",
    "\n",
    "df=pd.read_csv(\"/home/aakash/Documents/COding/BerFIneTuning/archive/all-data.csv\",encoding='ISO-8859-1')\n",
    "df.columns=['Sentiment','Sentence']\n",
    "# Example columns: 'Sentence', 'Sentiment'\n",
    "print(df.head())\n",
    "\n",
    "# Define the sentiment labels\n",
    "label_mapping = {\n",
    "    'positive': 0,\n",
    "    'negative': 1,\n",
    "    'neutral': 2\n",
    "}\n",
    "\n",
    "df['label'] = df['Sentiment'].map(label_mapping)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['Sentence'].values,\n",
    "    df['label'].values,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df['label'].values\n",
    ")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the texts\n",
    "max_length = 128  # Adjust based on your data\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(train_texts)\n",
    "val_encodings = tokenize_texts(val_texts)\n",
    "\n",
    "class FinancialPhraseDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: tensor[idx] for key, tensor in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = FinancialPhraseDataset(train_encodings, train_labels)\n",
    "val_dataset = FinancialPhraseDataset(val_encodings, val_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=3,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Initially freeze all BERT layers except the classification head\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define optimizer to include only trainable parameters\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Number of training steps\n",
    "epochs = 6  # Increased to allow for layer-wise unfreezing\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Define the scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),  # 10% warm-up\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function to unfreeze layers progressively\n",
    "def unfreeze_layers(model, epoch, total_epochs, total_layers):\n",
    "    \"\"\"\n",
    "    Unfreeze layers progressively based on the current epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The BertForSequenceClassification model\n",
    "    - epoch: Current epoch number\n",
    "    - total_epochs: Total number of epochs\n",
    "    - total_layers: Total number of BERT encoder layers\n",
    "    \"\"\"\n",
    "    layers_to_unfreeze = int((epoch / total_epochs) * total_layers)\n",
    "    if layers_to_unfreeze > 0:\n",
    "        for layer in model.bert.encoder.layer[-layers_to_unfreeze:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        print(f'\\nUnfroze the top {layers_to_unfreeze} layers.')\n",
    "\n",
    "# Training loop with layer-wise unfreezing\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f'\\n======== Epoch {epoch} / {epochs} ========')\n",
    "    print('Training...')\n",
    "    \n",
    "    # Unfreeze layers progressively\n",
    "    total_layers = len(model.bert.encoder.layer)\n",
    "    unfreeze_layers(model, epoch, epochs, total_layers)\n",
    "    \n",
    "    # Update optimizer to include newly trainable parameters\n",
    "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, eps=1e-8)\n",
    "    \n",
    "    # Reinitialize scheduler with the new optimizer\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),  # 10% warm-up\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Accumulate the training loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = batch['labels'].cpu().numpy()\n",
    "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_loader)\n",
    "    \n",
    "    print(f'\\nAverage training loss: {avg_train_loss:.4f}')\n",
    "    print(f'Average training accuracy: {avg_train_accuracy:.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    print('\\nRunning Validation...')\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_accuracy = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = batch['labels'].cpu().numpy()\n",
    "            \n",
    "            total_val_accuracy += flat_accuracy(logits, label_ids)\n",
    "            \n",
    "            all_preds.extend(np.argmax(logits, axis=1).flatten())\n",
    "            all_labels.extend(label_ids.flatten())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    avg_val_accuracy = total_val_accuracy / len(val_loader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "    print(f'Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_mapping.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
